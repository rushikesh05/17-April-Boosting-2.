{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Q1. What is Gradient Boosting Regression?\n",
        "\n",
        "##Ans:--\n",
        "\n",
        "###Gradient Boosting Regression is a popular machine learning algorithm used for regression tasks. It works by combining multiple weak predictive models, typically decision trees, to form a more accurate and robust model. The algorithm uses an iterative approach to minimize the error of the model, by fitting a new decision tree to the residual errors of the previous one.\n",
        "\n",
        "###At each iteration, the algorithm trains a new decision tree on the residuals of the previous one, with the goal of minimizing the loss function. The new tree is added to the ensemble, and the predictions are updated by adding the predictions of the new tree, weighted by a learning rate.\n",
        "\n",
        "###The learning rate is a hyperparameter that controls the contribution of each new tree to the final prediction. A lower learning rate will result in a more conservative update to the predictions, while a higher learning rate will result in a more aggressive update.\n",
        "\n",
        "###The algorithm continues to add new trees to the ensemble until a stopping criterion is met, such as reaching a maximum number of iterations or achieving a certain level of performance.\n",
        "\n",
        "###Gradient Boosting Regression is a powerful and flexible algorithm that can handle a wide range of regression problems, and it has been used successfully in many applications, including finance, healthcare, and natural language processing."
      ],
      "metadata": {
        "id": "oxy7ngRTiC9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n"
      ],
      "metadata": {
        "id": "xqxSgYDflGmB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfrNujhBh4vQ",
        "outputId": "9870b86d-01b7-4e79-84e2-a13abc555cad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 2.9958484523385316e-08\n",
            "R-squared: 0.9999999990921672\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.estimators = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.base_pred = np.mean(y)  # initial prediction is the mean of y\n",
        "        y_pred = np.full_like(y, self.base_pred).astype('float64')\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            residual = y - y_pred\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            tree.fit(X, residual)\n",
        "            self.estimators.append(tree)\n",
        "\n",
        "            # update predictions using learning rate and new estimator\n",
        "            y_pred += self.learning_rate * tree.predict(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.full(len(X), self.base_pred).astype('float64')\n",
        "        for estimator in self.estimators:\n",
        "            y_pred += self.learning_rate * estimator.predict(X)\n",
        "        return y_pred\n",
        "\n",
        "# Sample dataset\n",
        "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\n",
        "y = np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20])\n",
        "\n",
        "# Train model\n",
        "model = GradientBoostingRegressor()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Evaluate performance\n",
        "mse = np.mean((y - y_pred)**2)\n",
        "r_squared = 1 - np.sum((y - y_pred)**2) / np.sum((y - np.mean(y))**2)\n",
        "\n",
        "print(\"MSE:\", mse)\n",
        "print(\"R-squared:\", r_squared)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
      ],
      "metadata": {
        "id": "7_JQdO0LqSc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class GradientBoostingRegressor(BaseEstimator):\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.estimators = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.base_pred = np.mean(y)  # initial prediction is the mean of y\n",
        "        y_pred = np.full_like(y, self.base_pred).astype('float64')\n",
        "\n",
        "        for i in range(self.n_estimators):\n",
        "            residual = y - y_pred\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            tree.fit(X, residual)\n",
        "            self.estimators.append(tree)\n",
        "\n",
        "            # update predictions using learning rate and new estimator\n",
        "            y_pred += self.learning_rate * tree.predict(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.full(len(X), self.base_pred).astype('float64')\n",
        "        for estimator in self.estimators:\n",
        "            y_pred += self.learning_rate * estimator.predict(X)\n",
        "        return y_pred\n",
        "    \n",
        "    def get_params(self, deep=True):\n",
        "        return {\n",
        "            'n_estimators': self.n_estimators,\n",
        "            'learning_rate': self.learning_rate,\n",
        "            'max_depth': self.max_depth\n",
        "        }\n"
      ],
      "metadata": {
        "id": "zaMw7NPVlRKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid for grid search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'learning_rate': [0.1, 0.01, 0.001],\n",
        "    'max_depth': [2, 3, 4]\n",
        "}\n",
        "\n",
        "# Create grid search object\n",
        "grid_search = GridSearchCV(estimator=GradientBoostingRegressor(), param_grid=param_grid,\n",
        "                           scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit grid search object to data\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print results\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best negative MSE score:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piX6gWqbrGHM",
        "outputId": "61e0812e-8d3f-49df-a7c7-b09894c5f10d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 150}\n",
            "Best negative MSE score: -6.400001314158857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q4. What is a weak learner in Gradient Boosting?\n",
        "\n",
        "##Ans:--\n",
        "\n",
        "###In Gradient Boosting, a weak learner is a model that performs slightly better than random guessing on the training data. Typically, decision trees with low depth (also known as stumps) are used as weak learners in Gradient Boosting. The idea is to iteratively fit a series of weak learners to the residuals of the previous iteration, with each new weak learner learning to correct the errors of the previous ones. By combining the predictions of all weak learners, the final model is able to make accurate predictions on the training data.\n",
        "\n",
        "###Although each individual weak learner may not be very powerful, the ensemble of weak learners can create a strong learner that can generalize well to unseen data. The use of weak learners in Gradient Boosting allows for better control of overfitting and can lead to improved performance on a wide range of prediction problems."
      ],
      "metadata": {
        "id": "rObF8K-NsPX9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
        "\n",
        "##Ans:--\n",
        "\n",
        "###The intuition behind Gradient Boosting algorithm is to iteratively improve the predictions of a weak learning algorithm by combining it with a set of decision trees that correct the errors of the previous trees. The basic idea is to fit a model to the residuals of the previous model, such that each new model learns to correct the errors of the previous ones.\n",
        "\n",
        "###At each iteration, the model first makes a prediction on the data. The difference between the actual target values and the predicted values (the residuals) is then computed. A new model is then trained to predict the residuals, rather than the original target values. This new model is added to the ensemble, and its predictions are combined with the predictions of the previous models to produce the final prediction.\n",
        "\n",
        "###To prevent overfitting, each decision tree in the ensemble is typically a weak learner, which means that it is not powerful enough to make accurate predictions on its own. Instead, each tree is designed to capture only a small portion of the data's complexity, and by combining many such trees, the overall model can fit the data well while still avoiding overfitting.\n",
        "\n",
        "###The algorithm continues to add new trees until a stopping criterion is met, such as a maximum number of iterations or until the error on the training data stops decreasing. The final prediction is the sum of the predictions of all the trees in the ensemble.\n",
        "\n",
        "###Overall, Gradient Boosting is a powerful algorithm that can be used for a wide range of prediction problems. It has proven to be very effective in practice and is widely used in industry and academia."
      ],
      "metadata": {
        "id": "cjU7TMtDu5C0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
        "\n",
        "##Ans:--\n",
        "\n",
        "###Gradient Boosting algorithm builds an ensemble of weak learners by iteratively fitting a series of decision trees to the residuals of the previous trees. The basic steps of building an ensemble of weak learners using Gradient Boosting algorithm are as follows:\n",
        "\n",
        "* Initialize the ensemble by fitting a simple model to the data. This is usually a decision tree with a single node or a constant value.\n",
        "\n",
        "* Compute the residuals of the previous model by subtracting its predictions from the actual target values.\n",
        "\n",
        "* Fit a new decision tree to the residuals. This new tree will learn to correct the errors of the previous tree.\n",
        "\n",
        "* Add the new tree to the ensemble by combining its predictions with the predictions of the previous trees.\n",
        "\n",
        "* Repeat steps 2-4 until a stopping criterion is met. This stopping criterion could be a maximum number of iterations, a minimum improvement in performance, or other similar metrics.\n",
        "\n",
        "* Each decision tree in the ensemble is typically a weak learner, meaning that it is not powerful enough to make accurate predictions on its own. Instead, each tree is designed to capture only a small portion of the data's complexity, and by combining many such trees, the overall model can fit the data well while still avoiding overfitting.\n",
        "\n",
        "###The algorithm learns the weights for each weak learner through a process called gradient descent. It minimizes a loss function that measures the difference between the predictions of the model and the actual target values. The gradients of the loss function with respect to the predictions are used to update the weights of the weak learners in each iteration.\n",
        "\n",
        "###Overall, the Gradient Boosting algorithm builds an ensemble of weak learners by iteratively fitting a series of decision trees to the residuals of the previous trees and updating the weights of the ensemble through a process called gradient descent.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1cXOfN_qvC4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
        "\n",
        "##Ans:--\n",
        "\n",
        "###The mathematical intuition of Gradient Boosting algorithm involves several steps, which are as follows:\n",
        "\n",
        "* Define a loss function: The first step is to define a loss function that measures the difference between the predicted values and the actual target values. The loss function should be differentiable, as it will be used to update the weights of the weak learners.\n",
        "\n",
        "* Initialize the model: The next step is to initialize the model by setting the initial predictions to a constant value, such as the mean of the target values.\n",
        "\n",
        "* Compute the negative gradient: The negative gradient of the loss function with respect to the initial predictions is computed. This represents the direction in which the predictions need to be adjusted to minimize the loss function.\n",
        "\n",
        "* Fit a weak learner: A weak learner, typically a decision tree, is fit to the negative gradient. This tree is designed to capture the information in the data that is not explained by the current model.\n",
        "\n",
        "* Compute the step size: The step size is computed by minimizing the loss function along the direction of the weak learner. This step size determines the weight of the weak learner in the final model.\n",
        "\n",
        "* Update the model: The weak learner is added to the model by multiplying its predictions by the step size and adding them to the current predictions.\n",
        "\n",
        "* Repeat steps 3-6: The negative gradient is recomputed using the current predictions, and a new weak learner is fit to the negative gradient. This process is repeated until the desired number of weak learners is reached, or until the model converges.\n",
        "\n",
        "* Predictions: The final predictions are obtained by summing the predictions of all the weak learners in the ensemble.\n",
        "\n",
        "####Overall, the Gradient Boosting algorithm constructs the mathematical intuition by iteratively fitting weak learners to the negative gradient of the loss function, updating the model using the step size, and adding the weak learners to the ensemble. The process is repeated until the desired number of weak learners is reached, or until the model converges."
      ],
      "metadata": {
        "id": "8DNB5PM0wnGa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wfg3nPvqr8Vp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}